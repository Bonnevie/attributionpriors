{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example for Training with Attribution Priors\n",
    "This notebook is an example notebook for how to train with attribution priors. It defines some very simple, simulated data, and trains a small neural network to learn from that data. In the process, it shows how to regularize the neural network not to look at a specific feature using attribution priors. Although this particular example is very simple, attribution priors can be extended to more complex and useful penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import scipy.stats as stats\n",
    "from attributionpriors.ops import AttributionPriorExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to generate some random data to learn from\n",
    "- The data will have 1000 samples and 3 features.\n",
    "- The outcome $y$ will be continuous, $y = x_{0} - x_{1} + \\epsilon$ where $\\epsilon \\sim N(0, 0.5)$\n",
    "- The 3rd feature $x_{2}$ is a dummy feature, $x_2 = x_0 + N(0, 0.5)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate some random features\n",
    "feature_mean = 0.0\n",
    "feature_sigma = 1.0\n",
    "N = 1000\n",
    "d = 3\n",
    "X = np.random.randn(N, d) * feature_sigma + feature_mean\n",
    "X[:, 2] = X[:, 0] + np.random.randn(N) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_mean = 0.0\n",
    "outcome_sigma = 0.5\n",
    "Y = X[:, 0] - X[:, 1] + np.random.randn(N) * outcome_sigma + outcome_mean\n",
    "Y = np.reshape(Y, (1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Feature 0': X[:, 0], 'Feature 1': X[:, 1], 'Feature 2': X[:, 2], 'Outcome': Y.squeeze()})\n",
    "alt.Chart(data).mark_point(filled=True).encode(\n",
    "    x=alt.X(alt.repeat('column'), type='quantitative', scale=alt.Scale(domain=[-4, 4])),\n",
    "    y=alt.Y('Outcome:Q', scale=alt.Scale(domain=[-6, 6]))\n",
    ").properties(\n",
    "    height=200,\n",
    "    width=200\n",
    ").repeat(\n",
    "    column=['Feature 0', 'Feature 1', 'Feature 2']\n",
    ").properties(\n",
    "    title='The relationship between the outcome and the three features in our simulated data'\n",
    ").configure_axis(\n",
    "    labelFontSize=15,\n",
    "    labelFontWeight=alt.FontWeight('lighter'),\n",
    "    titleFontSize=15,\n",
    "    titleFontWeight=alt.FontWeight('normal')\n",
    ").configure_title(\n",
    "    fontSize=18\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splits + Data API\n",
    "Now we have some code to split the data into different chunks, and read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(X, Y, batch_size, shuffle=True, buffer_size=100):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_order = np.random.permutation(N)\n",
    "indices_train = index_order[:800]\n",
    "indices_vald  = index_order[800:900]\n",
    "indices_test  = index_order[900:]\n",
    "\n",
    "batch_size=20\n",
    "train_set = make_dataset(X[indices_train, :], Y[indices_train], batch_size=batch_size, shuffle=True, buffer_size=800)\n",
    "train_set = train_set.repeat(15)\n",
    "vald_set  = make_dataset(X[indices_vald, :],  Y[indices_vald],  batch_size=100, shuffle=False)\n",
    "test_set  = make_dataset(X[indices_test, :],  Y[indices_test],  batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, train_set.output_types, train_set.output_shapes)\n",
    "x_pl, y_true = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = train_set.make_initializable_iterator()\n",
    "vald_iter  = vald_set.make_initializable_iterator()\n",
    "test_iter  = test_set.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_handle = sess.run(train_iter.string_handle())\n",
    "vald_handle  = sess.run(vald_iter.string_handle())\n",
    "test_handle  = sess.run(test_iter.string_handle())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets build a model\n",
    "We will use a simple neural network with one hidden layer, and 5 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_pl):\n",
    "    hidden_units = tf.layers.Dense(5, activation=tf.nn.relu, use_bias=True)(x_pl)\n",
    "    output = tf.layers.Dense(1, activation=None, use_bias=False)(hidden_units)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_accuracy(labels, logits):\n",
    "    correct_prediction = tf.equal(tf.cast(tf.argmax(logits, 1), tf.int32), labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the API provided by wrapping our input using an `AttributionPriorExplainer` object, as explained\n",
    "in the README file of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AttributionPriorExplainer()\n",
    "cond_input_op, train_eg = explainer.input_to_samples_delta(x_pl)\n",
    "y_pred = model(cond_input_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_grads_op = explainer.shap_value_op(y_pred, cond_input_op, tf.squeeze(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that our expected gradients operation is the same shape\n",
    "as our input, because it should provide attributions for each feature in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_grads_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses\n",
    "We will use our standard mean squared error on the labels. As an attribution prior, we also penalize the squared\n",
    "magnitude of feature importance on the last feature. This will encourage our model not to use that last feature, which we know is a dummy feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_op = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(mse_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.0, 0.0, 1.0]).reshape(3, 1)\n",
    "tf_weights = tf.constant(weights, dtype=tf.float32)\n",
    "weighted_squared_loss = tf.matmul(tf.square(tf.cast(expected_grads_op, tf.float32)), tf_weights)\n",
    "weighted_squared_loss = tf.squeeze(weighted_squared_loss, axis=1)\n",
    "mean_weighted_squared_loss = tf.reduce_mean(weighted_squared_loss, name='mean_weighted_squared_loss')\n",
    "\n",
    "reg_lambda = tf.constant(30.0, dtype=tf.float32)\n",
    "reg_loss_op   = mean_weighted_squared_loss * reg_lambda\n",
    "\n",
    "train_eg_op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(reg_loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(use_ap):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(train_iter.initializer)\n",
    "    #One pass through the data:\n",
    "    i = 0\n",
    "    steps = []\n",
    "    validation_losses = []\n",
    "    while True:\n",
    "        try:\n",
    "            batch_input, batch_labels, _ = sess.run([cond_input_op, y_true, train_op], feed_dict={handle: train_handle})\n",
    "            if use_ap:\n",
    "                sess.run(train_eg_op, feed_dict={handle: train_handle, \n",
    "                                                 train_eg: True,\n",
    "                                                 x_pl: batch_input,\n",
    "                                                 y_true: batch_labels})\n",
    "            i += 1\n",
    "            if i % 10 == 0:\n",
    "                sess.run(vald_iter.initializer)\n",
    "                vald_loss = sess.run(mse_op, feed_dict={handle: vald_handle})\n",
    "                validation_losses.append(vald_loss)\n",
    "                steps.append(i)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "            \n",
    "    sess.run(test_iter.initializer)\n",
    "    test_loss= sess.run(mse_op, feed_dict={handle: test_handle})\n",
    "    \n",
    "    print('Test MSE: {:.4f}'.format(test_loss))\n",
    "    explainer = shap.GradientExplainer((cond_input_op, y_pred), X, sess)\n",
    "    shap_values = explainer.shap_values(X, nsamples=200)\n",
    "    shap.summary_plot(shap_values[0], X)\n",
    "    \n",
    "    sess.run(test_iter.initializer)\n",
    "    y_pred_test, y_true_test = sess.run([y_pred, y_true], feed_dict = {handle: test_handle})\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'Iteration': steps,\n",
    "        'Validation Loss': validation_losses\n",
    "    })\n",
    "    return alt.Chart(data).mark_line().encode(alt.X('Iteration:Q'), alt.Y('Validation Loss:Q', scale=alt.Scale(domain=[0.0, 2.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without an attribution prior\n",
    "First, we train without an attribution prior. The neural network mostly focuses on $x_0$ and $x_1$, but it also uses $x_2$ to make predictions, even though all the information contained in $x_2$ is already contained in $x_0$. We visualize the feature importances using the SHAP package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(use_ap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with an attribution prior\n",
    "Now we train with an attribution prior that pushes the model not to use $x_2$. In this case, the model no longer relies on $x_2$ and performs as well if not better than the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(use_ap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the session\n",
    "Don't forget to close the TensorFlow session when we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
